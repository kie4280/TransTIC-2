model: "tic_promptmodel_decoder"
training_data_path: /disk2/dataset/
training_dataset: [ "BSD400", "DIV2K", "Flickr2K", "flicker_2W_images", "WaterlooED"] # ["BSD400", "DIV2K", "Flickr2K", "WaterlooED", "flicker"]
testing_data_path: /disk2/dataset/test/
testing_dataset: ["Urban100"]
epochs: 80
learning_rate: 1.e-4
num_workers: 4
quality_level: 2 # {1,2,3,4}
lmbda: 0.0035 # {0.0018, 0.0035, 0.0067, 0.013}
batch_size: 8
test_batch_size: 1
aux_learning_rate: 1.e-3
patch_size: 256
cuda: True
gpu_id: 0
save: True
clip_max_norm: 1.0
root: "/disk2/finalproject_team22_record/"
name: "NAME"
exp_name: "Half_TransTIC_lamda0.0035"
seed: 42
checkpoint: '/disk2/finalproject_team22/TransTIC/TIC_weight/2/base_codec_2.pth.tar'
LOCATION: "prepend"
DEEP: True
NUM_TOKENS: 16
INITIATION: "random"
PROJECT: -1
DROPOUT: 0.
TRANSFER_TYPE: "prompt"
ARCHITECT: "both"
VPT_lmbda: 0.0035
WINDOW: "same"
HYPERPRIOR: False
RETURN_ATTENTION: False
MODEL_DECODER: False
MASK_DOWNSAMPLE: 2
DECODER_BLOCK: [1, 2, 3, 4]
TEST: False
noise_sigma: 25
milestones: [8, 16, 24]